{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5759ecb5",
   "metadata": {},
   "source": [
    "# The case of $q$-layer linear neural networks "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8749267c",
   "metadata": {},
   "source": [
    "First we define a function to get the gradients of the mapping parametrization $\\phi(U_1, \\cdots, U_q) = U_1 \\times \\cdots \\times U_q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a032fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_fields_for_q_layer_LNN(list_dim): #for example list_dim = [n1, n2, n3, n4]\n",
    "    \"\"\"\n",
    "    Returns D and the vector fields associated to ðœ™(ð‘ˆ1,â‹¯,ð‘ˆð‘ž)=ð‘ˆ1Ã—â‹¯Ã—ð‘ˆð‘ž\n",
    "    \"\"\"\n",
    "    D = int(np.array(list_dim)[:-1] @ np.array(list_dim)[1:].T)\n",
    "\n",
    "    list_var = [var('x'+str(i+1)) for i in range(D)]\n",
    "    R = PolynomialRing(ZZ, list_var) \n",
    "    \n",
    "    matrix_list = []\n",
    "    for i in range(len(list_dim)-1):\n",
    "        matrix = []\n",
    "        for j in range(list_dim[i]):\n",
    "            line = []\n",
    "            for k in range(list_dim[i+1]):\n",
    "                L = [0]*D\n",
    "                L[j * list_dim[i+1] + k + sum([list_dim[l] * list_dim[l+1] for l in range(i)])] = 1\n",
    "                line.extend([R.monomial(*L)])\n",
    "            matrix.append(line)\n",
    "        matrix_list.append(np.array(matrix))\n",
    "  \n",
    "    prod = np.linalg.multi_dot(matrix_list)\n",
    "    assert prod.shape == (list_dim[0], list_dim[-1])\n",
    "    vec = []\n",
    "    for i in range(list_dim[0]):\n",
    "        for j in range(list_dim[-1]):\n",
    "            vec.append(prod[i][j].gradient())\n",
    "    vec = np.array(vec).T\n",
    "    return D, vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d43683e8",
   "metadata": {},
   "source": [
    "And  we define a function that determines all independent conserved functions that are already known (and given by the Proposition 4.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_conserved_functions_linear(list_dim):\n",
    "    \"\"\"\n",
    "    Returns the number of independent conserved functions already known by the literature \n",
    "    for linear neural network case\n",
    "    \"\"\"\n",
    "    assert len(list_dim) > 2, \"Number of dimensions should be greater than 2\"\n",
    "    D = np.array(list_dim)[:-1] @ np.array(list_dim)[1:].T\n",
    "    list_var = [var('x'+str(i+1)) for i in range(D)]\n",
    "    R = PolynomialRing(ZZ, list_var) \n",
    "    \n",
    "    matrix_list = []\n",
    "    for i in range(len(list_dim)-1):\n",
    "        matrix = []\n",
    "        for j in range(list_dim[i]):\n",
    "            line = []\n",
    "            for k in range(list_dim[i+1]):\n",
    "                L = [0]*D\n",
    "                L[j * list_dim[i+1] + k + sum([list_dim[l] * list_dim[l+1] for l in range(i)])] = 1\n",
    "                line.extend([R.monomial(*L)])\n",
    "            matrix.append(line)\n",
    "        matrix_list.append(np.array(matrix))\n",
    "    \n",
    "    known_vec_fields = []\n",
    "    for i in range(len(list_dim)-2):\n",
    "        prod = matrix_list[i].T @ matrix_list[i] - matrix_list[i+1] @ matrix_list[i+1].T\n",
    "        for j in range(list_dim[i+1]):\n",
    "            for k in range(list_dim[i+1]):\n",
    "                known_vec_fields.append(prod[j][k].gradient())\n",
    "            \n",
    "    known_vec_fields = np.array(known_vec_fields).T\n",
    "    u, v = np.shape(known_vec_fields)\n",
    "    evaluation = np.empty((u, v))\n",
    "    value = np.random.rand(D)\n",
    "    dic = {}\n",
    "    for k in range(D):\n",
    "        dic[f\"x{k+1}\"] = value[k]\n",
    "    for i in range(u):\n",
    "        for j in range(v):\n",
    "            evaluation[i, j] = known_vec_fields[i][j](**dic)\n",
    "    rank = np.linalg.matrix_rank(evaluation)\n",
    "    return rank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a93f9e09",
   "metadata": {},
   "source": [
    "# The case of $q$-layer ReLU neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e58bf3d",
   "metadata": {},
   "source": [
    "First we define a function to get the gradients of the mapping parametrization $\\phi(U_1, \\cdots, U_q, b_1, \\cdots, b_{q-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f40e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sage.parallel.decorate import parallel\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Create function for operations inside first loop\n",
    "@parallel\n",
    "def compute_gradient_1(dims, matrix_list, bias_list, num_mat):\n",
    "    vec = []\n",
    "    to_multiply = [matrix_list[0].copy()[:, dims[0]]]\n",
    "    for i in range(len(dims) - 1):\n",
    "        to_multiply.append(np.array(matrix_list[i + 1].copy()[dims[i], dims[i+1]]))\n",
    "    to_multiply.append(np.array(bias_list.copy()[num_mat - 1][dims[-1]]))\n",
    "    prod = np.array(product(to_multiply)).reshape(-1)\n",
    "    for l in range(len(prod)):\n",
    "        vec.append(prod[l].gradient())\n",
    "    return vec\n",
    "\n",
    "# Create function for operations inside second loop (last layer)\n",
    "@parallel\n",
    "def compute_gradient_2(dims, matrix_list):\n",
    "    vec = []\n",
    "    to_multiply = [matrix_list[0].copy()[:, dims[0]]]\n",
    "    for i in range(len(dims) - 1):\n",
    "        to_multiply.append(np.array(matrix_list[i + 1].copy()[dims[i], dims[i+1]]))\n",
    "    prod = np.array(product(to_multiply)).reshape(-1, 1) @ np.array(matrix_list[-1].copy()[dims[-1], :]).reshape(1, -1)\n",
    "    for l in range(prod.shape[0]):\n",
    "        for m in range(prod.shape[1]):\n",
    "            vec.append(prod[l][m].gradient())\n",
    "    return vec\n",
    "\n",
    "def vector_fields_ReLU_q_layers(list_dim, bias = False):\n",
    "    \n",
    "    D = np.array(list_dim)[:-1] @ np.array(list_dim)[1:].T\n",
    "    dim_weights = D\n",
    "    \n",
    "    if bias:\n",
    "        D += sum([list_dim[l] for l in range(1, len(list_dim)-1)])\n",
    "        \n",
    "    list_var = [var('x'+str(i+1)) for i in range(D)]\n",
    "    R = PolynomialRing(ZZ, list_var) \n",
    "    \n",
    "    # definition of the weight matrices\n",
    "    matrix_list = []\n",
    "    for i in range(len(list_dim)-1):\n",
    "        matrix = []\n",
    "        for j in range(list_dim[i]):\n",
    "            line = []\n",
    "            for k in range(list_dim[i+1]):\n",
    "                L = [0]*D\n",
    "                L[j * list_dim[i+1] + k + sum([list_dim[l] * list_dim[l+1] for l in range(i)])] = 1\n",
    "                line.extend([R.monomial(*L)])\n",
    "            matrix.append(line)\n",
    "        matrix_list.append(np.array(matrix))\n",
    "    \n",
    "    # definition of the bias\n",
    "    if bias:\n",
    "        bias_list = []\n",
    "        for i in range(1, len(list_dim)-1):\n",
    "            line = []\n",
    "            for j in range(list_dim[i]):\n",
    "                L = [0]*D\n",
    "                L[dim_weights + j + sum([list_dim[l] for l in range(1, i)])] += 1\n",
    "                line.extend([R.monomial(*L)])\n",
    "            bias_list.append(line)\n",
    "    \n",
    "        \n",
    "    # Now use joblib to parallelize the for-loops\n",
    "    vec = []\n",
    "    if bias:\n",
    "        for num_mat in range(1, len(list_dim) - 1):\n",
    "            dims_list = itertools.product(*[list(range(list_dim[i])) for i in range(1, num_mat+1)])\n",
    "            # use sage parallelization\n",
    "            results = compute_gradient_1([(dims, matrix_list, bias_list, num_mat) for dims in dims_list])\n",
    "            vec += [item for sublist in results for item in sublist[1]]  # Flatten results\n",
    "\n",
    "    dims_list = itertools.product(*[list(range(list_dim[i])) for i in range(1, len(list_dim) - 1)])\n",
    "    # use sage parallelization\n",
    "    results = compute_gradient_2([(dims, matrix_list) for dims in dims_list])\n",
    "    vec += [item for sublist in results for item in sublist[1]]  # Flatten results\n",
    "    \n",
    "    return D, np.array(vec).T\n",
    "    \n",
    "D, vector_fields = vector_fields_ReLU_q_layers([2, 2, 5, 3], bias=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f962d7b",
   "metadata": {},
   "source": [
    "And  we define a function that determines all independent conserved functions that are already known (and given by the Proposition 4.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_conserved_functions_ReLU(list_dim):\n",
    "    return sum([list_dim[i] for i in range(1, len(list_dim)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f212b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.8",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
